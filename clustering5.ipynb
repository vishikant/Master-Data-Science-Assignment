{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917c20f-fec2-4a8f-a970-88948ab545d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?\n",
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "used to evaluate the performance of language models?\n",
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?\n",
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?\n",
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?\n",
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7893-9de5-4775-96b0-e32bb240a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Q1.** A **contingency matrix** (or confusion matrix) is a table used to evaluate the performance of a classification model by comparing actual versus predicted classes. Each row represents actual classes, and each column represents predicted classes. It helps calculate metrics like accuracy, precision, recall, and F1-score, providing insights into model performance.\n",
    "\n",
    "**Q2.** A **pair confusion matrix** evaluates clustering by comparing pairs of data points, assessing whether pairs are placed in the same or different clusters compared to the ground truth. Unlike a regular confusion matrix, it focuses on pairwise relationships, which is useful for clustering tasks where labels aren't directly compared.\n",
    "\n",
    "**Q3.** An **extrinsic measure** in natural language processing evaluates a language models performance based on how well it performs on an external task (e.g., translation, sentiment analysis). It reflects the model's real-world utility.\n",
    "\n",
    "**Q4.** An **intrinsic measure** evaluates a models performance based on internal metrics (e.g., perplexity, clustering quality). Unlike \n",
    "extrinsic measures, it doesnt rely on downstream tasks but focuses on the model's internal performance characteristics.\n",
    "\n",
    "**Q5.** The **confusion matrix** in machine learning provides a detailed breakdown of a models performance by showing true positives, false positives, true negatives, and false negatives. It identifies strengths (e.g., high recall) and weaknesses (e.g., high false positive rate), guiding model improvement.\n",
    "\n",
    "**Q6.** Common **intrinsic measures** for unsupervised learning include the **Silhouette Score** (measuring cluster separation), **Davies-Bouldin Index** (evaluating cluster compactness), and **Calinski-Harabasz Index** (assessing cluster dispersion). They help gauge clustering quality without labeled data.\n",
    "\n",
    "**Q7.** **Accuracy** as a sole metric can be misleading in imbalanced datasets, where it may mask poor performance on minority classes. For example, in a dataset with 90% of one class, predicting only that class yields 90% accuracy but fails to capture the minority class. Address this by using metrics like precision, recall, F1-score, and the confusion matrix to evaluate model performance across all classes. These metrics provide a more nuanced view of how well the model performs, especially in scenarios where class imbalance is significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
