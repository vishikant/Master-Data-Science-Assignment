{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does itrepresent?\n",
    "R-squared (R2) is a statistical measure that represents the proportion of the variance in the dependent \n",
    "variable that is explained by the independent variables in a linear regression model. It is a key indicator of\n",
    "the goodness of fit of the model. In simpler terms, R2 tells us how well the independent variables predict the dependent variable.\n",
    "\n",
    "R-squared is a useful statistic for understanding the explanatory power of a linear regression model. It quantifies \n",
    "how much of the variance in the dependent variable is explained by the independent variables, helping to assess the \n",
    "goodness of fit of the model. However, it should be used in conjunction with other metrics and diagnostic tools to \n",
    "ensure a comprehensive evaluation of the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "Adjusted R2 accounts for the number of predictors in the model and provides a more accurate measure when \n",
    "comparing models with different numbers of independent variables.Adjusted R2\n",
    "penalizes the addition of non-significant predictors, providing a better measure of model quality. where as\n",
    "R-squared is a useful statistic for understanding the explanatory power of a linear regression model. It quantifies\n",
    "how much of the variance in the dependent variable is explained by the independent variables, helping to assess the \n",
    "goodness of fit of the model. However, it should be used in conjunction with other metrics and diagnostic tools to\n",
    "ensure a comprehensive evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Adjusted R-squared is more appropriate to use when comparing regression models with different numbers of predictors.\n",
    "Unlike R-squared, which can artificially increase as more predictors are added regardless of their relevance, \n",
    "adjusted R-squared accounts for the number of predictors and only increases if the new predictors improve the \n",
    "model's fit. This makes it a better measure for evaluating the true explanatory power of the model, especially \n",
    "in cases where the risk of overfitting is high. By penalizing the addition of irrelevant predictors, adjusted\n",
    "R-squared provides a more accurate assessment of model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "#calculated, and what do they represent?\n",
    "### Mean Squared Error (MSE)\n",
    " MSE measures the average squared difference between actual and predicted values.\n",
    "- Calculation: \n",
    "  \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\]\n",
    "- **Representation**: Indicates the average magnitude of errors, heavily penalizing larger errors.\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "- Definition: RMSE is the square root of MSE.\n",
    "- **Calculation**:\n",
    "  \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} \\]\n",
    "- **Representation**: Provides error magnitude in the same units as the dependent variable, easier to interpret.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "- Definition: MAE measures the average absolute difference between actual and predicted values.\n",
    "- **Calculation**:\n",
    "  \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| \\]\n",
    "- **Representation**: Reflects average error magnitude without squaring, less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "#regression analysis.\n",
    "### RMSE and MSE\n",
    "**Advantages**:\n",
    "- Penalize larger errors more heavily, making them useful for identifying significant discrepancies.\n",
    "- RMSE provides error magnitude in the same units as the dependent variable.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Sensitive to outliers, which can distort model evaluation.\n",
    "- MSE can be harder to interpret due to squared units.\n",
    "\n",
    "### MAE\n",
    "**Advantages**:\n",
    "- Less sensitive to outliers, providing a more robust measure of average error.\n",
    "- Easier to interpret as it represents average error in the same units as the dependent variable.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Treats all errors equally, which might not capture the impact of larger errors effectively.\n",
    "- Does not penalize large errors as strongly as MSE/RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lasso Regularization\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty equal to the absolute value\n",
    "of the coefficients' magnitudes to the loss function:\n",
    "\\[ \\text{Lasso Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n",
    "where \\(\\lambda\\) controls the penalty's strength.\n",
    "\n",
    "### Difference from Ridge Regularization\n",
    "Ridge regularization uses the sum of squared coefficients:\n",
    "\\[ \\text{Ridge Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "\n",
    "### Appropriateness\n",
    "Lasso is more appropriate when feature selection is needed, as it can shrink some coefficients to exactly zero,\n",
    "effectively selecting a simpler model. Ridge is preferred when all predictors should be kept and multicollinearity \n",
    "is an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "#example to illustrate.\n",
    "Regularized linear models, like Lasso and Ridge regression, prevent overfitting by adding a penalty to the loss \n",
    "function that constrains the size of the model coefficients. This discourages the model from fitting the noise in \n",
    "the training data, promoting simpler, more generalizable models. \n",
    "\n",
    "### Example\n",
    "Consider a linear regression model with many predictors. Without regularization, the model might fit the training\n",
    "data perfectly but perform poorly on new data (overfitting). Adding a Ridge penalty (\\(\\lambda \\sum \\beta_j^2\\))\n",
    "shrinks coefficients, reducing the model's complexity and improving its performance on unseen data by focusing on \n",
    "the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "#choice for regression analysis.\n",
    "Regularized linear models, such as Lasso and Ridge regression, have limitations. They assume a linear relationship \n",
    "between predictors and the outcome, which may not capture complex, non-linear patterns in the data. Regularization \n",
    "can also lead to biased estimates, especially if the penalty parameter is not appropriately tuned. In scenarios \n",
    "with highly correlated predictors, Lasso might randomly select one predictor over another, which can be unstable.\n",
    "Moreover, regularized models may not perform well with small datasets or when the true signal is weak, as the\n",
    "penalty can dominate and obscure meaningful relationships. Hence, they might not always be the best choice for all \n",
    "regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5029689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the better performer between Model A (RMSE = 10) and Model B (MAE = 8) depends on the context and the specific evaluation criteria:\n",
    "\n",
    "- **Model B (MAE = 8)**: Indicates a lower average absolute error, suggesting it performs better in general accuracy.\n",
    "- **Model A (RMSE = 10)**: Higher RMSE might indicate it handles larger errors worse.\n",
    "\n",
    "However, RMSE penalizes larger errors more heavily, so if minimizing large errors is crucial, Model A's RMSE should be considered. \n",
    "\n",
    "### Limitations\n",
    "- **RMSE Sensitivity**: RMSE is more sensitive to outliers.\n",
    "- **MAE Equivalence**: MAE treats all errors equally, potentially ignoring the impact of larger errors.\n",
    "\n",
    "Thus, the choice of metric depends on whether you prioritize average error magnitude (MAE) or heavily penalizing larger errors (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing between Model A (Ridge, \\(\\lambda = 0.1\\)) and Model B (Lasso, \\(\\lambda = 0.5\\)) depends on the specific goals and characteristics of the data:\n",
    "\n",
    "- **Ridge Regularization (Model A)**: Tends to keep all predictors, shrinking coefficients evenly. Better when all\n",
    "    features are believed to contribute to the outcome.\n",
    "- **Lasso Regularization (Model B)**: Can set some coefficients to zero, effectively performing feature selection.\n",
    "    Useful when you suspect many features are irrelevant.\n",
    "\n",
    "### Trade-offs and Limitations\n",
    "- **Ridge**: Doesn't perform feature selection, which can be a drawback in high-dimensional data.\n",
    "- **Lasso**: Can be unstable with highly correlated predictors and might exclude important features.\n",
    "- **Regularization Parameter**: Different \\(\\lambda\\) values make direct comparison challenging. \n",
    "    Model performance should be evaluated based on cross-validation or validation set results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
