{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09766791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a \n",
    "regularization term proportional to the absolute values of the coefficients:\n",
    "\n",
    "This penalty can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "### Differences from Other Techniques:\n",
    "- **Ordinary Least Squares (OLS)**: Minimizes only the residual sum of squares without regularization.\n",
    "- **Ridge Regression**: Uses a squared penalty (\\(\\lambda \\sum \\beta_j^2\\)), shrinking coefficients but not setting any to zero.\n",
    "- **Elastic Net**: Combines Lasso and Ridge penalties, providing a balance between feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec69c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a \n",
    "regularization term proportional to the absolute values of the coefficients\n",
    "\n",
    "This penalty can shrink some coefficients to exactly zero, effectively performin feature selection.\n",
    "\n",
    "Differences from Other Techniques:\n",
    "Ordinary Least Squares (OLS): Minimizes only the residual sum of squares without regularization.\n",
    "Ridge Regression: Uses a squared penalty, shrinking coefficients but not setting any to zero.\n",
    "Elastic Net: Combines Lasso and Ridge penalties, providing a balance between feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b268c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Interpreting the coefficients of a Lasso Regression model involves understanding both their magnitude and their potential sparsity:\n",
    "\n",
    "1. **Magnitude**: Each non-zero coefficient \\(\\beta_j\\) represents the estimated change in the dependent variable for a one-unit change in the corresponding predictor, holding other predictors constant. The sign (positive or negative) indicates the direction of the relationship.\n",
    "\n",
    "2. **Sparsity**: Due to the \\( \\ell_1 \\) regularization, Lasso can shrink some coefficients to exactly zero. A coefficient of zero indicates that the corresponding predictor is excluded from the model, signifying that it is not considered important for predicting the outcome.\n",
    "\n",
    "In essence, non-zero coefficients show the relevant predictors and their impact, while zero coefficients highlight features deemed irrelevant by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect themodel's performance?\n",
    "In Lasso Regression, the primary tuning parameter is the regularization parameter \\(\\lambda\\). This parameter controls the strength of the penalty applied to the coefficients:\n",
    "\n",
    "1. **Regularization Parameter (\\(\\lambda\\))**:\n",
    "   - **Effect on Performance**:\n",
    "     - **High \\(\\lambda\\)**: Increases the penalty, leading to more coefficients being shrunk to zero, enhancing feature selection and reducing model complexity. However, if \\(\\lambda\\) is too high, it can oversimplify the model, potentially underfitting the data.\n",
    "     - **Low \\(\\lambda\\)**: Reduces the penalty, resulting in less shrinkage of coefficients. This can capture more features, but may lead to overfitting if irrelevant features are included.\n",
    "   \n",
    "Selecting the optimal \\(\\lambda\\) value is crucial and is typically done using cross-validation. By balancing bias and variance, an appropriate \\(\\lambda\\) ensures the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ea919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Yes, Lasso Regression can be adapted for non-linear regression problems by incorporating non-linear transformations\n",
    "of the predictors. This involves creating new features through polynomial expansion, interaction terms, or other \n",
    "non-linear transformations such as logarithmic or exponential functions. By transforming the predictors, Lasso can \n",
    "capture non-linear relationships between the independent and dependent variables. However, as with any regression \n",
    "technique, proper feature engineering and regularization parameter selection are crucial for effectively modeling\n",
    "non-linear relationships and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ee632",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques for linear regression, but they differ primarily in the type of penalty they apply to the coefficients:\n",
    "\n",
    "1. **Penalty Type**:\n",
    "   - **Ridge Regression**: Adds a penalty proportional to the squared magnitude of coefficients (\\( \\lambda \\sum \\beta_j^2 \\)), promoting shrinkage but not setting any coefficients exactly to zero.\n",
    "   - **Lasso Regression**: Adds a penalty proportional to the absolute magnitude of coefficients (\\( \\lambda \\sum |\\beta_j| \\)), promoting shrinkage and effectively performing feature selection by setting some coefficients to zero.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Ridge tends to shrink coefficients evenly, while Lasso can effectively exclude some predictors entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9751b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Yes, Lasso Regression can handle multicollinearity, although not as effectively as Ridge Regression.\n",
    "Multicollinearity occurs when predictors are highly correlated, leading to unstable coefficient estimates in \n",
    "ordinary least squares (OLS) regression. \n",
    "\n",
    "Lasso addresses multicollinearity by automatically performing feature selection, shrinking coefficients towards \n",
    "zero and setting some coefficients to exactly zero. By effectively excluding some predictors from the model, \n",
    "Lasso reduces the impact of multicollinearity. However, it's worth noting that Lasso might not completely eliminate \n",
    "multicollinearity-related issues, especially if predictors are highly correlated. In such cases, Ridge Regression \n",
    "might be more suitable for mitigating multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97392e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The optimal value of the regularization parameter (\\(\\lambda\\)) in Lasso Regression is typically chosen using \n",
    "cross-validation. This involves:\n",
    "1. Defining a range of \\(\\lambda\\) values.\n",
    "2. Performing k-fold cross-validation, partitioning the data into k subsets.\n",
    "3. Training the model on \\(k-1\\) subsets and validating it on the remaining subset.\n",
    "4. Repeating this process for each \\(\\lambda\\) value and evaluating performance metrics, such as mean squared error or \\(R^2\\).\n",
    "5. Selecting the \\(\\lambda\\) value that minimizes the cross-validated error, ensuring the best balance between bias and \n",
    "variance.#Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
