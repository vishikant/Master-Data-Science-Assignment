{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32538f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "#example of each.\n",
    "If there is a single input variable X(independent variable), such linear regression is simple linear regression. \n",
    "where as multiple linear regression is have more than one independent feature\n",
    "Example of simple linear Regression is having data of student studying time and we need to predict\n",
    "if they going to pass or not.\n",
    "Multiple Linear Regression is having data of house size and area , we need to predict price of the house\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold ina given dataset?\n",
    "Linearity: The relationship between the independent variables and the dependent variable is linear.\n",
    "    This means the model correctly specifies that the change in the dependent variable is a linear function \n",
    "    of the independent variables.\n",
    "\n",
    "Independence: The residuals (errors) are independent. This implies that the observations themselves are also \n",
    "    independent. There should be no correlation between consecutive errors, which is particularly relevant in \n",
    "    time series data.\n",
    "\n",
    "Homoscedasticity: The residuals have constant variance at every level of the independent variables. In other words,\n",
    "    the spread or variability of the residuals is consistent across all levels of the independent variables.\n",
    "\n",
    "Normality: The residuals of the model are normally distributed. This assumption is particularly important for\n",
    "    hypothesis testing and constructing confidence intervals.\n",
    "\n",
    "No Multicollinearity: The independent variables are not perfectly linearly related. High multicollinearity means \n",
    "    that one independent variable can be linearly predicted from the others with a substantial degree of accuracy, \n",
    "    which can inflate the variance of coefficient estimates and make the model unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "#3a real-world scenario.\n",
    " The slope represents the change in the dependent variable for each unit change in the independent variable, while \n",
    "    the intercept represents the predicted value of the dependent variable when the independent variable is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and \n",
    "neural networks. It trains machine learning models by minimizing errors between predicted and actual results.\n",
    "\n",
    "Initialize Parameters: Start with random initial values for the model's parameters.\n",
    "    \n",
    "Compute the Gradient: Calculate the gradient of the loss function with respect to each parameter. \n",
    "    This involves taking partial derivatives of the loss function.\n",
    "    \n",
    "Update Parameters: Adjust the parameters in the direction opposite to the gradient (because we want to minimize the\n",
    "    loss, not increase it). The size of the adjustment is controlled by a factor called the learning rate.\n",
    "    \n",
    "Iterate: Repeat the process of computing the gradient and updating the parameters until the algorithm converges\n",
    "    to a minimum, meaning the changes in the loss become very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "If there is a single input variable X(independent variable), such linear regression is simple linear regression. \n",
    "where as multiple linear regression is have more than one independent feature\n",
    "Example of simple linear Regression is having data of student studying time and we need to predict\n",
    "if they going to pass or not.\n",
    "Multiple Linear Regression is having data of house size and area , we need to predict price of the house\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "#address this issue?\n",
    "\n",
    "Multicollinearity in multiple linear regression refers to a situation where two or more independent variables \n",
    "(predictors) are highly correlated. This high correlation means that one predictor can be linearly predicted \n",
    "from the others with a substantial degree of accuracy.\n",
    "\n",
    "Correlation Matrix: Calculate the Pearson correlation coefficients between all pairs of independent variables. \n",
    "    High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "Example: A correlation coefficient greater than 0.8 or less than -0.8 can be a red flag.\n",
    "    \n",
    "Eigenvalues and Condition Number: Analyzing the eigenvalues of the correlation matrix of the predictors. \n",
    "    A condition number (ratio of the largest to the smallest eigenvalue) greater than 30 indicates potential \n",
    "    multicollinearity.\n",
    "    \n",
    "To detect multicolinearity\n",
    "Remove Highly Correlated Predictors: One of the simplest methods is to remove one of the highly correlated \n",
    "    variables from the model. This reduces redundancy and simplifies the model.\n",
    "\n",
    "Combine Predictors: Combine correlated predictors into a single predictor through techniques like Principal \n",
    "    Component Analysis (PCA) or creating an index.\n",
    "\n",
    "Regularization Techniques: Apply regularization methods such as Ridge Regression (L2 regularization) or Lasso\n",
    "    Regression (L1 regularization). These techniques add a penalty to the regression model, shrinking the \n",
    "    coefficients of less important predictors:\n",
    "\n",
    "Ridge Regression: Shrinks all coefficients but does not eliminate any.\n",
    "Lasso Regression: Can shrink some coefficients to zero, effectively selecting a simpler model with fewer predictors.\n",
    "Increase Sample Size: In some cases, increasing the sample size can help mitigate the effects of multicollinearity, \n",
    "    although this is not always feasible.\n",
    "\n",
    "Centering Predictors: Subtracting the mean of each predictor from its values (centering) can sometimes reduce\n",
    "    multicollinearity, particularly when there is interaction between predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "# regression? In what situations would you prefer to use polynomial regression?\n",
    "Polynomial regression is an extension of linear regression where the relationship between the independent variable \n",
    "x\n",
    "x and the dependent variable \n",
    "y\n",
    "y is modeled as an \n",
    "n\n",
    "n-th degree polynomial. \n",
    "\n",
    "Differences from Linear Regression\n",
    "Nature of the Relationship: Linear regression models a straight-line relationship\n",
    " whereas polynomial regression models a curvilinear relationship.\n",
    "Flexibility: Polynomial regression can capture more complex, non-linear patterns in the data compared to the single linear term in linear regression.\n",
    "Complexity: Polynomial regression introduces additional polynomial terms, increasing the model's complexity and the risk of overfitting.\n",
    "Q8. Advantages and Disadvantages of Polynomial Regression\n",
    "Advantages\n",
    "\n",
    "Flexibility: Polynomial regression can model complex, non-linear relationships between variables, making it more flexible than linear regression.\n",
    "Better Fit for Curved Data: It provides a better fit for datasets where the relationship between the dependent and independent variables is inherently non-linear.\n",
    "Modeling Peaks and Troughs: Can capture local minima and maxima, which are not possible with linear regression.\n",
    "Disadvantages\n",
    "\n",
    "Overfitting: Higher-degree polynomials can lead to overfitting, where the model captures noise in the data rather than the underlying trend.\n",
    "Extrapolation Issues: Polynomial regression can produce unrealistic predictions outside the range of the training data.\n",
    "Complexity and Interpretability: As the degree of the polynomial increases, the model becomes more complex and harder to interpret.\n",
    "Computational Cost: Higher-degree polynomials increase the computational cost and may require more resources for training and evaluation.\n",
    "When to Use Polynomial Regression\n",
    "Polynomial regression is preferable when the relationship between the dependent and independent variables is non-linear and cannot be adequately captured by a simple linear model. It is particularly useful when there are observable curvatures in the data, such as quadratic or cubic trends. For example, in fields like economics, biology, or engineering, where relationships between variables often exhibit non-linear patterns, polynomial regression can provide a more accurate and insightful model. However, care must be taken to avoid overfitting by choosing an appropriate degree for the polynomial and using techniques like cross-validation to assess model performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
