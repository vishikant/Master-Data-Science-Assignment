{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae848a-eb77-4dde-bdfb-11c65de3ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q1: Min-Max Scaling\n",
    "**Min-Max scaling** is a feature scaling technique that transforms features to a fixed range, typically [0, 1]. It rescales each feature to ensure that the minimum value becomes 0 and the maximum becomes 1.\n",
    "### Q1: Min-Max Scaling\n",
    "**Min-Max scaling** is a feature scaling technique that transforms features to a fixed range, typically [0, 1]. It rescales each feature to ensure that the minimum value becomes 0 and the maximum becomes 1.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "X' = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n",
    "\\]\n",
    "\n",
    "**Example:** \n",
    "Consider a dataset with a feature that has values [10, 20, 30]. Using Min-Max scaling:\n",
    "- Min = 10, Max = 30\n",
    "- Scaled values:\n",
    "  - For 10: \\((10 - 10) / (30 - 10) = 0\\)\n",
    "  - For 20: \\((20 - 10) / (30 - 10) = 0.5\\)\n",
    "  - For 30: \\((30 - 10) / (30 - 10) = 1\\)\n",
    "  \n",
    "Thus, the scaled values are [0, 0.5, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Unit Vector Technique\n",
    "The **Unit Vector technique** (or normalization) scales the feature vector to have a length of 1. It is calculated by dividing each component of the vector by its magnitude.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "\\text{Unit Vector} = \\frac{X}{||X||}\n",
    "\\]\n",
    "Where \\( ||X|| \\) is the Euclidean norm of vector \\( X \\).\n",
    "\n",
    "**Example:** \n",
    "For a feature vector \\( [3, 4] \\):\n",
    "- Magnitude = \\( \\sqrt{3^2 + 4^2} = 5 \\)\n",
    "- Unit Vector = \\( [3/5, 4/5] = [0.6, 0.8] \\)\n",
    "\n",
    "**Difference from Min-Max Scaling:** Min-Max scales data to a fixed range, while the Unit Vector technique normalizes the vector to a unit length.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: Principal Component Analysis (PCA)\n",
    "**PCA** is a dimensionality reduction technique that transforms a dataset into a set of orthogonal (uncorrelated) components that capture the maximum variance in the data.\n",
    "\n",
    "**Example:**\n",
    "Given a dataset with features such as height and weight, PCA can reduce these two dimensions into a single principal component that retains the most variance. This can be particularly useful in visualizing high-dimensional data or speeding up model training by reducing complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: PCA and Feature Extraction\n",
    "**PCA** is closely related to **feature extraction** as it transforms the original features into a new set of features (principal components) that can capture the essential patterns in the data.\n",
    "\n",
    "**Example:**\n",
    "In a dataset with many correlated features (like height, weight, age), PCA can extract the top principal components that summarize the variance. If the first two components explain 90% of the variance, you might choose to retain them for further analysis or modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: Min-Max Scaling in Recommendation System\n",
    "In a food delivery service dataset with features like price, rating, and delivery time, Min-Max scaling can be applied to ensure all features contribute equally to the model:\n",
    "- Scale price, rating, and delivery time to [0, 1].\n",
    "- This helps the model learn more effectively by eliminating the bias that could arise from features being on different scales.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6: PCA for Dimensionality Reduction in Stock Price Prediction\n",
    "In predicting stock prices with various features (like financial metrics), PCA can reduce dimensionality by:\n",
    "1. Standardizing the dataset to have a mean of 0 and a standard deviation of 1.\n",
    "2. Applying PCA to transform the original features into principal components.\n",
    "3. Retaining the components that explain the most variance (e.g., 95% of variance) for model training, simplifying the model without significant loss of information.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7: Min-Max Scaling to Range -1 to 1\n",
    "To scale the values [1, 5, 10, 15, 20] to a range of [-1, 1]:\n",
    "\n",
    "**Step 1:** Apply Min-Max scaling to [0, 1]:\n",
    "- Min = 1, Max = 20\n",
    "- Scaled values:\n",
    "  - For 1: \\((1 - 1) / (20 - 1) = 0\\)\n",
    "  - For 5: \\((5 - 1) / (20 - 1) = 0.2105\\)\n",
    "  - For 10: \\((10 - 1) / (20 - 1) = 0.4737\\)\n",
    "  - For 15: \\((15 - 1) / (20 - 1) = 0.7368\\)\n",
    "  - For 20: \\((20 - 1) / (20 - 1) = 1\\)\n",
    "\n",
    "**Step 2:** Scale to [-1, 1]:\n",
    "\\[\n",
    "X = 2 \\cdot \\text{scaled\\_value} - 1\n",
    "\\]\n",
    "Resulting in values:\n",
    "- For 1: -1\n",
    "- For 5: -0.5789\n",
    "- For 10: -0.0526\n",
    "- For 15: 0.4737\n",
    "- For 20: 1\n",
    "\n",
    "---\n",
    "\n",
    "### Q8: PCA Feature Extraction for Given Features\n",
    "For a dataset with features [height, weight, age, gender, blood pressure], PCA can be used to extract principal components. \n",
    "\n",
    "**How Many Components to Retain:**\n",
    "The number of components to retain would depend on the explained variance. Typically, components that together explain a threshold (e.g., 90% of the variance) would be retained. If the first three principal components explain 95% of the variance, you would keep those for further analysis or modeling, as they provide a good representation of the original features while reducing dimensionality.\n",
    "**Formula:**\n",
    "\\[\n",
    "X' = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n",
    "\\]\n",
    "\n",
    "**Example:** \n",
    "Consider a dataset with a feature that has values [10, 20, 30]. Using Min-Max scaling:\n",
    "- Min = 10, Max = 30\n",
    "- Scaled values:\n",
    "  - For 10: \\((10 - 10) / (30 - 10) = 0\\)\n",
    "  - For 20: \\((20 - 10) / (30 - 10) = 0.5\\)\n",
    "  - For 30: \\((30 - 10) / (30 - 10) = 1\\)\n",
    "  \n",
    "Thus, the scaled values are [0, 0.5, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Unit Vector Technique\n",
    "The **Unit Vector technique** (or normalization) scales the feature vector to have a length of 1. It is calculated by dividing each component of the vector by its magnitude.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "\\text{Unit Vector} = \\frac{X}{||X||}\n",
    "\\]\n",
    "Where \\( ||X|| \\) is the Euclidean norm of vector \\( X \\).\n",
    "\n",
    "**Example:** \n",
    "For a feature vector \\( [3, 4] \\):\n",
    "- Magnitude = \\( \\sqrt{3^2 + 4^2} = 5 \\)\n",
    "- Unit Vector = \\( [3/5, 4/5] = [0.6, 0.8] \\)\n",
    "\n",
    "**Difference from Min-Max Scaling:** Min-Max scales data to a fixed range, while the Unit Vector technique normalizes the vector to a unit length.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3: Principal Component Analysis (PCA)\n",
    "**PCA** is a dimensionality reduction technique that transforms a dataset into a set of orthogonal (uncorrelated) components that capture the maximum variance in the data.\n",
    "\n",
    "**Example:**\n",
    "Given a dataset with features such as height and weight, PCA can reduce these two dimensions into a single principal component that retains the most variance. This can be particularly useful in visualizing high-dimensional data or speeding up model training by reducing complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4: PCA and Feature Extraction\n",
    "**PCA** is closely related to **feature extraction** as it transforms the original features into a new set of features (principal components) that can capture the essential patterns in the data.\n",
    "\n",
    "**Example:**\n",
    "In a dataset with many correlated features (like height, weight, age), PCA can extract the top principal components that summarize the variance. If the first two components explain 90% of the variance, you might choose to retain them for further analysis or modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5: Min-Max Scaling in Recommendation System\n",
    "In a food delivery service dataset with features like price, rating, and delivery time, Min-Max scaling can be applied to ensure all features contribute equally to the model:\n",
    "- Scale price, rating, and delivery time to [0, 1].\n",
    "- This helps the model learn more effectively by eliminating the bias that could arise from features being on different scales.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6: PCA for Dimensionality Reduction in Stock Price Prediction\n",
    "In predicting stock prices with various features (like financial metrics), PCA can reduce dimensionality by:\n",
    "1. Standardizing the dataset to have a mean of 0 and a standard deviation of 1.\n",
    "2. Applying PCA to transform the original features into principal components.\n",
    "3. Retaining the components that explain the most variance (e.g., 95% of variance) for model training, simplifying the model without significant loss of information.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7: Min-Max Scaling to Range -1 to 1\n",
    "To scale the values [1, 5, 10, 15, 20] to a range of [-1, 1]:\n",
    "\n",
    "**Step 1:** Apply Min-Max scaling to [0, 1]:\n",
    "- Min = 1, Max = 20\n",
    "- Scaled values:\n",
    "  - For 1: \\((1 - 1) / (20 - 1) = 0\\)\n",
    "  - For 5: \\((5 - 1) / (20 - 1) = 0.2105\\)\n",
    "  - For 10: \\((10 - 1) / (20 - 1) = 0.4737\\)\n",
    "  - For 15: \\((15 - 1) / (20 - 1) = 0.7368\\)\n",
    "  - For 20: \\((20 - 1) / (20 - 1) = 1\\)\n",
    "\n",
    "**Step 2:** Scale to [-1, 1]:\n",
    "\\[\n",
    "X' = 2 \\cdot \\text{scaled\\_value} - 1\n",
    "\\]\n",
    "Resulting in values:\n",
    "- For 1: -1\n",
    "- For 5: -0.5789\n",
    "- For 10: -0.0526\n",
    "- For 15: 0.4737\n",
    "- For 20: 1\n",
    "\n",
    "---\n",
    "\n",
    "### Q8: PCA Feature Extraction for Given Features\n",
    "For a dataset with features [height, weight, age, gender, blood pressure], PCA can be used to extract principal components. \n",
    "\n",
    "**How Many Components to Retain:**\n",
    "The number of components to retain would depend on the explained variance. Typically, components that together explain a threshold (e.g., 90% of the variance) would be retained. If the first three principal components explain 95% of the variance, you would keep those for further analysis or modeling, as they provide a good representation of the original features while reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc776b47-c8f3-4038-826b-91e9e72e017e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
